name: Censio Stack Events Archive
run-name: ${{ format('{0} - {1}', github.workflow, github.event_name == 'schedule' && 'Nightly Run' || 'Manual Run') }}
on:
  workflow_dispatch:
    inputs:
      max_days:
        description: "The maximum number of days to export"
        required: false
        type: string
        default: "30"
  schedule:
    # Follows POSIX cron syntax https://pubs.opengroup.org/onlinepubs/9699919799/utilities/crontab.html#tag_20_25_07
    # * is a special character in YAML, so must be quoted
    # Run at 10:25 UTC every Sunday which is 04:25 CST/05:25 CDT
    - cron: '00 11 * * 0'

env:
  TABLE_NAME: CENSIO_STACK
  BQ_DATASET_NAME: stack

jobs:
  Mysql_To_BigQuery_Sync:
    name: Archive MySQL data to BigQuery
    uses: ./.github/workflows/archive.yml
    with:
      bq_dataset_name: stack
      mysql_database: "opengamedata"
      mysql_table_name: CENSIO_STACK
      max_days: ${{ github.event.inputs.max_days || 7 }}
    secrets: inherit

  Cleanup_Synced_Rows:
    name: Cleanup Synced Rows
    runs-on: ubuntu-24.04
    needs: Mysql_To_BigQuery_Sync

    steps:

    # 1. Remote config 
    - name: Install OpenConnect
      run: sudo apt-get update && sudo apt-get install openconnect
    - name: Connect to VPN
      run: echo ${{ secrets.VPN_PASS }} | sudo openconnect --protocol=gp -u ${{ secrets.VPN_USER }} --passwd-on-stdin soe.vpn.wisc.edu &
    - name: Setup server key # TODO: figure out/set up key secret for logging in to logger host
      run: |
        mkdir -p ~/.ssh
        echo '${{secrets.SERVICE_KEY}}' >> ./key.txt
        chmod 600 ./key.txt

    # 2. Cleanup
    - name: Run delete of archived data at least a week old
      run:
        ssh -o StrictHostKeyChecking=no -i ./key.txt ${{secrets.VPN_USER}}@${{vars.OGD_LOGGER_HOST}} "mysql -u${{secrets.OGD_ARCHIVING_USER}} -p${{secrets.OGD_ARCHIVING_PASS}} -e 'delete from opengamedata.${{ env.TABLE_NAME }} where synced=1 and datediff(now(), server_time) > 7'"
